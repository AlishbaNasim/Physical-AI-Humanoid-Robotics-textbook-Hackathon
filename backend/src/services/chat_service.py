from typing import List, Dict, Any
from ..models.chat import ChatMessage, ContentSource
from .vector_service import VectorService
from .embedding_service import EmbeddingService
from .database_service import DatabaseService
import uuid
import datetime
import os
from openai import AsyncOpenAI
import logging

class ChatService:
    def __init__(self):
        self.vector_service = VectorService()
        self.embedding_service = EmbeddingService()
        self.database_service = DatabaseService()
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY")) if os.getenv("OPENAI_API_KEY") else None

    async def process_chat(self, message: str, selected_text: str = "", session_id: str = None, user_id: str = None) -> Dict[str, Any]:
        """Process a chat message and return a response with sources"""
        if not session_id:
            session_id = str(uuid.uuid4())

        # Save the user message to the database
        await self.database_service.save_message(session_id, "user", message)

        # Search for relevant content in the vector database
        search_results = await self.vector_service.search_similar(message, limit=5)

        # Format sources
        sources = []
        for result in search_results:
            sources.append(ContentSource(
                content_id=result["content_id"],
                title=result["title"],
                url=result["url"],
                text=result["text"],
                relevance_score=result["relevance_score"]
            ))

        # Generate response using OpenAI with RAG
        response_text = await self._generate_response(message, selected_text, search_results)

        # Save the assistant message to the database
        await self.database_service.save_message(session_id, "assistant", response_text, [s.dict() for s in sources])

        # Save or update the session
        await self.database_service.save_session(session_id, user_id)

        return {
            "response": response_text,
            "sources": [s.dict() for s in sources],  # Convert to dict for JSON serialization
            "session_id": session_id,
            "timestamp": datetime.datetime.now().isoformat()
        }

    async def _generate_response(self, message: str, selected_text: str, context: List[Dict[str, Any]]) -> str:
        """Generate a response using OpenAI with the provided context"""
        if not self.openai_client:
            # Placeholder response when OpenAI is not available
            context_text = " ".join([item["text"] for item in context[:2]])  # Use first 2 results
            return f"Based on the book content: '{context_text[:200]}...', I understand your question about '{message[:50]}...'. This is a placeholder response. In a real implementation, this would be generated by OpenAI using the book content as context."

        try:
            # Prepare the context for the LLM
            context_text = "\n\n".join([f"Source: {item['title']}\nContent: {item['text']}" for item in context])

            # If selected text is provided, include it in the prompt
            if selected_text:
                context_text = f"User selected text: {selected_text}\n\n{context_text}"

            # Create the system message to guide the assistant
            system_message = """You are an AI assistant for the Physical AI & Humanoid Robotics book.
            Answer questions based only on the provided book content.
            If the answer is not in the provided context, say so clearly.
            Always cite your sources and be helpful but factual."""

            # Create the user message
            user_message = f"Question: {message}\n\nContext:\n{context_text}"

            # Call the OpenAI API
            response = await self.openai_client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.3,
                max_tokens=500
            )

            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"Error generating response with OpenAI: {e}")
            # Fallback response
            context_text = " ".join([item["text"] for item in context[:2]])  # Use first 2 results
            return f"Based on the book content: '{context_text[:200]}...', I understand your question about '{message[:50]}...'. This is a fallback response due to an API error."

    async def embed_content(self, content_id: str, title: str, content: str, module: str, metadata: Dict[str, Any] = None) -> str:
        """Embed content into the vector database"""
        vector_id = await self.vector_service.add_content(content_id, title, content, metadata)
        return vector_id